[["index.html", "Comparing AI and Hand Counts of Deer Chapter 1 Introduction 1.1 Project Goals 1.2 Project Outline", " Comparing AI and Hand Counts of Deer Courtney Check 2021-04-27 Chapter 1 Introduction As part of my MS Thesis, I will need to count deer across several thousand camera trap photos. To speed this up, someone developed an AI that will supposedly count deer in the photos for us. However, before we adopt the AI, I want to compare the AI’s counts to our handmade counts, and see if there are any site-level differences in its counting ability. This book documents my evaluation of the AI’s effectiveness. 1.1 Project Goals The goals of this project are to: Check the agreement between the hand counts of deer and the AI counts of deer Check if there are site-level discrepancies in the AI counts Identify problem sites 1.2 Project Outline Chapter 1: Introduction Chapter 2: Database Creation Chapter 3: Checking Overall Count Agreement Chapter 4: Checking Site-level Count Agreement Chapter 5: Conclusion "],["database.html", "Chapter 2 Database Creation 2.1 Getting Necessary Packages 2.2 Load the Data 2.3 Clean the Data 2.4 Create a New, Empty SQL Database 2.5 Append the Cleaned Data to the Empty SQL Database", " Chapter 2 Database Creation For my research, I have deployed 107 camera traps. This generates a lot of images, all of which must be individually evaluated so that the animals in the photos can be identified and counted. Recently, we gained access to an AI that can supposedly count deer for us, but we want to compare the AI counts to our own handmade counts before adopting it. In this document, I will be creating a database of a) the hand deer counts, b) the AI deer counts and c) the site info so that I can compare how well the AI did compared to counts made by technicians. I’ve decided to arrange the database as follows: The highlighted items are the primary and foreign keys for each dataframe. The black lines show how the foreign keys will connect the dataframes to each other. Site_id is the primary key for the site info, since each row contains info on a unique site, and it can also be used as foreign key to link it to the count dataframes. Jpeg_name is the primary key for both count dataframes, because of their rows contain data for a single unique jpg. Jpeg_name can also serve as a foreign key to connect the count dataframes to each other, since both dataframes contain info on the same set of jpegs. 2.1 Getting Necessary Packages 2.1.1 Install DBI I am trying to install a database in SQL, so I will need the DBI package. install.packages(&quot;RSQLite&quot;) install.packages(&quot;DBI&quot;) 2.1.2 Calling packages into R In addition to loading DBI, I also need tidyverse and lubridate to clean the data. library(DBI) library(tidyverse) library(lubridate) 2.2 Load the Data The full database of all the AI and hand counted photos is apparently too big to work with as just 2 files, so I have subsetted it into seasons. I have included just one season/year (summer 2019) to show my process in a way that doesn’t have too much repetition, though it works them same for all the season/years. I am loading three csvs: the hand counts of deer, the ai counts of deer, and the information on each site. hand &lt;- read.csv(&quot;/Users/courtney/Desktop/MS_Thesis/Data/Compiled-Raw/summer19.csv&quot;, stringsAsFactors = FALSE) ai &lt;- read.csv(&quot;/Users/courtney/Desktop/MS_Thesis/Data/Compiled-Raw/AI.summer19.csv&quot;, stringsAsFactors = FALSE) site &lt;- read.csv(&quot;/Users/courtney/Desktop/MS_Thesis/Data/Compiled-Raw/site_info.csv&quot;, stringsAsFactors = FALSE) 2.3 Clean the Data First, I clean the hand count data. To do this, I coerce the photo date/photo time columns to be date and time objects, respectively. I also pull out only the columns I want, to leave out redundant categories like organism family, order, class, etc. and rename them so that common columns/keys will match with the other dataframes. hand.clean &lt;- hand %&gt;% mutate(dt = ymd_hms(paste(Photo.Date, Photo.time))) %&gt;% select(jpg_name = Raw.Name, site_id = Camera.Trap.Name, species = Species, sampling_event = Sampling.Event, photo_type = Photo.Type, photo_date = Photo.Date, photo_time = Photo.time, n_all = Number.of.Animals, person_identifying = Person.Identifying.the.Photo, camera_start_date = Camera.Start.Date, camera_end_date = Camera.End.Date) %&gt;% # Mule deer are tagged with species as &#39;hemionus&#39; mutate(n_animals = case_when( species == &quot;hemionus&quot; | species == &quot;guttata&quot; ~ n_all, TRUE ~ 0L )) %&gt;% # Drop n_animals and species select(-n_all) Next, I clean the AI count data. Once again, I pull out only the columns I want and leave out extra unnccessary columns like the file path to the jpg, etc. and rename them so that common columns/keys will match with the other dataframes. ai.clean &lt;- ai %&gt;% select(jpg_name = Raw_Name, site_id = Camera.Trap.Name, hand_animal_present = hand_label_has_animal, ai_animal_present = det_has_animal, ai_count = n_pred_deer) Lastly, I do modify the “site_id” category so that it is consistent across all three databases. Because the “site” dataframe just includes site as a single number, I remove all the “SITE” characters from the hand count and AI count dataframes. hand.clean$site_id = gsub(&quot; &quot;, &quot;&quot;, hand.clean$site_id) hand.clean$site_id = gsub(&quot;Site&quot;, &quot;&quot;, hand.clean$site_id) ai.clean$site_id = gsub(&quot;site&quot;, &quot;&quot;, ai.clean$site_id) ai.clean$site_id = gsub(&quot;00&quot;, &quot;&quot;, ai.clean$site_id) ai.clean$site_id = gsub(&quot;(?&lt;!\\\\d)0&quot;, &quot;&quot;, ai.clean$site_id, perl = TRUE) 2.4 Create a New, Empty SQL Database Here, I create an empty SQL database using the DBI package that I can later populate. counts_db &lt;- dbConnect(RSQLite::SQLite(), &quot;/Users/courtney/Desktop/MS_Thesis/Data/SQL_db.db&quot;) 2.5 Append the Cleaned Data to the Empty SQL Database First, I create the database tables, specificing the primary and foreign keys for each one. Site Info Table: dbExecute(counts_db, &quot;CREATE TABLE site_info ( site_id double(3) NOT NULL, camera_id double(3) NOT NULL, sd_id varchar(5) NOT NULL, lat double(20), long double(20), mount char(100), landmark char(100), PRIMARY KEY (site_id) );&quot;) AI Count Table: dbExecute(counts_db, &quot;CREATE TABLE ai_counts ( jpg_name varchar(100) NOT NULL, site_id double(3) NOT NULL, hand_animal_present char(10), ai_animal_present char(10), ai_count double(10), PRIMARY KEY (jpg_name) FOREIGN KEY (jpg_name) REFERENCES hand_counts(jpg_name) FOREIGN KEY (site_id) REFERENCES site_info(site_id) );&quot;) Hand Count Table: dbExecute(counts_db, &quot;CREATE TABLE hand_counts ( jpg_name varchar(100) NOT NULL, site_id double(3) NOT NULL, species char(50), sampling_event varchar(50), photo_type varchar(50), photo_date varchar(50), photo_time varchar(50), person_identifying varchar(50), camera_start_date varchar(50), camera_end_date varchar(50), n_animals double(10), PRIMARY KEY (jpg_name) FOREIGN KEY (jpg_name) REFERENCES ai_counts(jpg_name) FOREIGN KEY (site_id) REFERENCES site_info(site_id) );&quot;) Then, I add all of the cleaned dataframes into the SQL database. Database complete! I also check to make sure that the dataframes were actually added by calling their first ten rows to look at. dbWriteTable(counts_db, &quot;site_info&quot;, site, append = TRUE) dbGetQuery(counts_db, &quot;SELECT * FROM site_info LIMIT 10;&quot;) dbWriteTable(counts_db, &quot;ai_counts&quot;, ai.clean, append = TRUE) dbGetQuery(counts_db, &quot;SELECT * FROM ai_counts LIMIT 10;&quot;) dbWriteTable(counts_db, &quot;hand_counts&quot;, hand.clean, append = TRUE) dbGetQuery(counts_db, &quot;SELECT * FROM hand_counts LIMIT 10;&quot;) "],["overall.html", "Chapter 3 Checking Overall Agreement", " Chapter 3 Checking Overall Agreement First, I want to get a sense of the overall agreement between hand counts and AI counts. This will give an idea of the total amount of data we might be losing to the AI algorithm. To do this, I first have to calculate the difference between the AI and hand counts. hand.counts &lt;- dbGetQuery(counts_db, &quot;SELECT * FROM hand_counts;&quot;) ai.counts &lt;- dbGetQuery(counts_db, &quot;SELECT * FROM ai_counts;&quot;) ai_hand &lt;- hand.counts %&gt;% inner_join(ai.counts, by = c(&quot;jpg_name&quot;, &quot;site_id&quot;)) %&gt;% mutate(diff = n_animals - ai_count) Next, I look at the mean, standard deviation, and range of difference between AI and hand counts. m = mean(ai_hand$diff) s = sd(ai_hand$diff) r = range(ai_hand$diff) Here’s the Mean: ## [1] 0.005713605 Here’s the Standard Deviation: ## [1] 0.1933531 Here’s the Range: ## [1] -4 8 From this, I can see that the mean and sd are very low, but the range shows that there are still some pretty big miscounts by the AI. So I also want to check how often the AI makes “big” miscalculations. “Big” in this case, is any instance of &gt; +/-1 deer. ai_hand %&gt;% group_by(diff) %&gt;% summarize(freq = n()) %&gt;% mutate(percent = round((freq/sum(freq)*100))) %&gt;% rename(&quot;Count Difference&quot; = diff, &quot;Frequency&quot; = freq, &quot;Percent of Observations&quot; = percent) %&gt;% knitr::kable(align = &quot;c&quot;) Count Difference Frequency Percent of Observations -4 1 0 -3 7 0 -2 47 0 -1 1399 1 0 130883 97 1 1604 1 2 250 0 3 39 0 4 4 0 5 1 0 6 2 0 7 1 0 8 3 0 Looks like most of the AI’s counts are within the +/-1 deer range! The big miscounts make up only less than one percent of the counts. Overall, agreement seems pretty good! The mean and sd are very low, and even though there some big miscounts, they are a very small portion of all the counts. "],["site.html", "Chapter 4 Checking Site-level Agreement", " Chapter 4 Checking Site-level Agreement Even though overall agreement is really good, it’s possible that there are some sites where the AI is less effective because of the terrain, shade level, etc. Because our sites don’t change, if I identify problem sites now, I can know to not trust the AI’s counts and to hand–count these sites in the future. First things first: I want to check the min, max, and mean of count difference for each site. site_diff &lt;- ai_hand %&gt;% group_by(site_id) %&gt;% summarize(n = n(), min_diff = min(diff), mean_diff = mean(diff), max_diff = max(diff)) site_diff %&gt;% arrange(desc(max_diff)) %&gt;% slice_head(n = 20) %&gt;% rename(Site = site_id, &quot;Number of Observations&quot; = n, Min_Diff = min_diff, Mean_Diff = mean_diff, Max_Diff = max_diff) %&gt;% knitr::kable(align = &quot;c&quot;) Site Number of Observations Min_Diff Mean_Diff Max_Diff 74 1585 -3 0.1249211 8 8 312 -2 0.1089744 3 41 14694 -4 0.0596162 3 68 946 0 0.0528541 3 82 191 -1 0.1413613 3 86 2224 -1 0.0193345 3 25 11352 -1 -0.0069591 2 40 2112 -1 -0.0066288 2 42 2229 -2 -0.0116644 2 44 4905 -3 -0.0101937 2 75 1812 -1 -0.0005519 2 79 2828 -1 0.0042433 2 87 2582 -1 0.0290473 2 90 2379 -1 0.0369903 2 98 1234 -1 -0.0008104 2 1 622 -1 0.0209003 1 5 171 -1 0.0175439 1 7 489 -1 -0.0020450 1 9 4666 -3 -0.0025718 1 10 3546 -1 -0.0016920 1 Definitely a lot of variation! Some sites are perfect, others are in between, and one site (Site 74) has the biggest over-count AND nearly the biggest under-count. To help better get a grasp of the variation, I’m going to plot it by site. site_diff %&gt;% arrange(desc(site_id)) %&gt;% ggplot(aes(x = factor(site_id), y = mean_diff)) + geom_point() + geom_errorbar(aes(ymin = min_diff, ymax = max_diff)) + labs(title = &quot;Comparison of AI Counts to Hand Counts by Site&quot;, x = &quot;Site&quot;, y = &quot;Difference (Hand - AI)&quot;) + theme_light() + theme(plot.title = element_text(size = 19, face = &quot;bold&quot;, hjust = 0.5), axis.title = element_text(size = 11), axis.text.x = element_text(size = 7, angle = 90, vjust = 0.5)) While this is a good eyeball assessment, I want to firmly identify “problem” sites. A “problem” site in this case is going to be defined as any site that correctly counts &lt;90% of all photos. ai_hand %&gt;% group_by(site_id, diff) %&gt;% summarize(freq = n()) %&gt;% mutate(percent = round((freq/sum(freq)*100), digits = 2) ) %&gt;% filter(max(percent) &lt; 90) %&gt;% top_n(1, percent) %&gt;% select(site_id, percent) %&gt;% rename(Site = site_id, &quot;Percent Correct AI Counts&quot; = percent) %&gt;% knitr::kable(align = &quot;c&quot;) Site Percent Correct AI Counts 8 77.88 20 88.89 74 87.95 82 89.01 Only four sites! That’s not bad. And most of them hover around 90% agreement, with only one site (Site 8) sporting a truly abysmal 77.88% agreement. Now that I know these sites are difficult for the AI to count deer at, I can be sure to check their counts by hand going forward. "],["conclusion.html", "Chapter 5 Conclusion", " Chapter 5 Conclusion Overall, the AI seems to perform pretty well! While there are a handful of sites that will still need to be hand-counted, the AI will still be able to save us a lot of time without sacrificing much data! Yay! "]]
